{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b6f1365",
   "metadata": {},
   "source": [
    "# LangChain Notebook (Part 1): Readers + `Document` object + Cleaning (in depth)\n",
    "\n",
    "This notebook focuses on **only**:\n",
    "1. **Readers / Loaders** → how data becomes LangChain `Document` objects (with metadata)\n",
    "2. **Cleaning** → practical, production-style text cleanup **while preserving metadata**\n",
    "\n",
    "You can run this notebook end-to-end on your machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7369c29a",
   "metadata": {},
   "source": [
    "## 0) Install & Imports\n",
    "\n",
    "> If you already have LangChain installed, you can skip the install cell.\n",
    "\n",
    "We'll use:\n",
    "- `langchain-core` for the `Document`\n",
    "- `langchain-community` for common loaders like `TextLoader`, `DirectoryLoader`, `PyPDFLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28aae4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready ✅\n"
     ]
    }
   ],
   "source": [
    "# If needed (run once)\n",
    "# %pip install -U langchain langchain-core langchain-community pypdf unstructured\n",
    "\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Iterable, Callable\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Loaders (Readers)\n",
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "\n",
    "# Optional: PDF loader (needs pypdf)\n",
    "try:\n",
    "    from langchain_community.document_loaders import PyPDFLoader\n",
    "except Exception:\n",
    "    PyPDFLoader = None\n",
    "\n",
    "print(\"Ready ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8ba82d",
   "metadata": {},
   "source": [
    "## 1) What is a LangChain `Document`?\n",
    "\n",
    "A `Document` is the **standard container** LangChain uses across loaders, splitters, vector DBs, retrievers, etc.\n",
    "\n",
    "It has two main fields:\n",
    "- `page_content`: the text content\n",
    "- `metadata`: a dictionary (source, file path, page number, etc.)\n",
    "\n",
    "**Why metadata matters (production):**\n",
    "- You need to show citations (\"source: file.pdf page 3\")\n",
    "- You need filtering (\"only docs from customer=A\")\n",
    "- You need traceability & debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a26abdc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Document(metadata={'source': 'manual', 'topic': 'demo'}, page_content='Hello! This is a sample document.'),\n",
       " 'Hello! This is a sample document.',\n",
       " {'source': 'manual', 'topic': 'demo'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = Document(\n",
    "    page_content=\"Hello! This is a sample document.\",\n",
    "    metadata={\"source\": \"manual\", \"topic\": \"demo\"}\n",
    ")\n",
    "\n",
    "doc, doc.page_content, doc.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5427d2d",
   "metadata": {},
   "source": [
    "## 2) Create a small local dataset (for demo)\n",
    "\n",
    "We'll create a mini folder with a few `.txt` files containing:\n",
    "- extra whitespace\n",
    "- weird unicode characters\n",
    "- repeated “header/footer” like PDF exports\n",
    "- some boilerplate lines\n",
    "\n",
    "So we can practice **realistic cleaning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cff9c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created files: ['doc1.txt', 'doc3.txt', 'doc2.txt']\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"demo_docs\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "(DATA_DIR / \"doc1.txt\").write_text(\n",
    "\"\"\"ACME SUPPORT PORTAL — INTERNAL USE ONLY\n",
    "------------------------------------------\n",
    "\n",
    "Hello   team,\n",
    "\n",
    "This   is    a    test document.  \n",
    "\n",
    "It contains    extra spaces,    odd line breaks,\n",
    "and some unicode like café, naïve, and “smart quotes”.\n",
    "\n",
    "ACME SUPPORT PORTAL — INTERNAL USE ONLY\n",
    "Page 1\n",
    "\"\"\", \n",
    "encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "(DATA_DIR / \"doc2.txt\").write_text(\n",
    "\"\"\"ACME SUPPORT PORTAL — INTERNAL USE ONLY\n",
    "------------------------------------------\n",
    "\n",
    "FAQ:\n",
    "1) Reset password — go to Settings → Security.\n",
    "2) Contact support at support@example.com\n",
    "\n",
    "Disclaimer: This email and any attachments are confidential.\n",
    "Disclaimer: This email and any attachments are confidential.\n",
    "\n",
    "ACME SUPPORT PORTAL — INTERNAL USE ONLY\n",
    "Page 2\n",
    "\"\"\", \n",
    "encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "(DATA_DIR / \"doc3.txt\").write_text(\n",
    "\"\"\"Report Title: Quarterly Summary\n",
    "\n",
    "\\t\\tThis line starts with tabs.\n",
    "\\n\\n\\nMultiple blank lines above.\n",
    "\n",
    "• Bullet 1\n",
    "• Bullet 2\n",
    "\n",
    "Footer: Company Confidential\n",
    "Footer: Company Confidential\n",
    "Footer: Company Confidential\n",
    "\"\"\", \n",
    "encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "print(\"Created files:\", [p.name for p in DATA_DIR.glob(\"*.txt\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de886426",
   "metadata": {},
   "source": [
    "## 3) Reader 1: `TextLoader` (single file)\n",
    "\n",
    "`TextLoader` turns one file into a list of `Document` objects (usually 1 document for a txt file).\n",
    "\n",
    "Check how metadata is stored (typically includes the file path as `source`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42350385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " {'source': 'demo_docs/doc1.txt'},\n",
       " 'ACME SUPPORT PORTAL — INTERNAL USE ONLY\\n------------------------------------------\\n\\nHello   team,\\n\\nThis   is    a    test document.  \\n\\nIt contains    extra spaces,    odd line breaks,\\nand some unicode')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_docs = TextLoader(str(DATA_DIR / \"doc1.txt\"), encoding=\"utf-8\").load()\n",
    "len(single_docs), single_docs[0].metadata, single_docs[0].page_content[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cde398",
   "metadata": {},
   "source": [
    "## 4) Reader 2: `DirectoryLoader` (many files)\n",
    "\n",
    "`DirectoryLoader` is used for ingesting a folder.\n",
    "- `glob=\"**/*.txt\"` picks matching files recursively\n",
    "- `loader_cls=TextLoader` tells it how to load each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a33a1a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents loaded: 3\n",
      "--- Doc 1 ---\n",
      "source: demo_docs/doc1.txt\n",
      "chars: 287\n",
      "preview: 'ACME SUPPORT PORTAL — INTERNAL USE ONLY\\n------------------------------------------\\n\\nHello   team,\\n\\nThis   is    a    tes'\n",
      "--- Doc 2 ---\n",
      "source: demo_docs/doc3.txt\n",
      "chars: 205\n",
      "preview: 'Report Title: Quarterly Summary\\n\\n\\t\\tThis line starts with tabs.\\n\\n\\n\\nMultiple blank lines above.\\n\\n• Bullet 1\\n• Bullet 2\\n\\nFo'\n",
      "--- Doc 3 ---\n",
      "source: demo_docs/doc2.txt\n",
      "chars: 349\n",
      "preview: 'ACME SUPPORT PORTAL — INTERNAL USE ONLY\\n------------------------------------------\\n\\nFAQ:\\n1) Reset password — go to Setti'\n"
     ]
    }
   ],
   "source": [
    "dir_loader = DirectoryLoader(\n",
    "    str(DATA_DIR),\n",
    "    glob=\"**/*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\": \"utf-8\"}\n",
    ")\n",
    "\n",
    "docs: List[Document] = dir_loader.load()\n",
    "print(\"Documents loaded:\", len(docs))\n",
    "\n",
    "# Show a quick summary\n",
    "for i, d in enumerate(docs, start=1):\n",
    "    print(f\"--- Doc {i} ---\")\n",
    "    print(\"source:\", d.metadata.get(\"source\"))\n",
    "    print(\"chars:\", len(d.page_content))\n",
    "    print(\"preview:\", repr(d.page_content[:120]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9e2d25",
   "metadata": {},
   "source": [
    "## 5) Optional Reader: `PyPDFLoader` (PDF → per-page Documents)\n",
    "\n",
    "If you load a PDF, you usually get **one Document per page**, with metadata like:\n",
    "- `source` (file path)\n",
    "- `page` (page number)\n",
    "\n",
    "> This section is optional; you can run it if you have a PDF file locally.\n",
    "\n",
    "Example usage:\n",
    "\n",
    "```python\n",
    "pdf_docs = PyPDFLoader(\"myfile.pdf\").load()\n",
    "pdf_docs[0].metadata  # includes page\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe384c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Only run if you have a PDF file.\n",
    "# Put a pdf path below and uncomment.\n",
    "\n",
    "# PDF_PATH = \"sample.pdf\"\n",
    "# if PyPDFLoader is None:\n",
    "#     print(\"PyPDFLoader not available. Install pypdf: %pip install -U pypdf\")\n",
    "# else:\n",
    "#     pdf_docs = PyPDFLoader(PDF_PATH).load()\n",
    "#     print(\"PDF pages loaded:\", len(pdf_docs))\n",
    "#     print(\"First page metadata:\", pdf_docs[0].metadata)\n",
    "#     print(pdf_docs[0].page_content[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd5392f",
   "metadata": {},
   "source": [
    "# Cleaning (in depth)\n",
    "\n",
    "Cleaning is not “one perfect function.” In production, you build a **pipeline**:\n",
    "1. Normalize unicode (quotes, weird spacing)\n",
    "2. Standardize whitespace\n",
    "3. Remove boilerplate (disclaimers, repeated banners)\n",
    "4. Remove repeated headers/footers (common in PDFs)\n",
    "5. Deduplicate repeated lines\n",
    "6. Keep metadata + add cleaning metadata for traceability\n",
    "\n",
    "We'll implement each step as a **small function** and compose them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a4fcff",
   "metadata": {},
   "source": [
    "## 6) Step A — Unicode normalization\n",
    "\n",
    "Why:\n",
    "- Different sources contain fancy quotes, non-breaking spaces, unusual dashes\n",
    "- Normalization makes text consistent for embeddings and retrieval\n",
    "\n",
    "We’ll use:\n",
    "- `unicodedata.normalize(\"NFKC\", text)`\n",
    "- Replace non-breaking spaces with normal spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44baee71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE: café “smart quotes” with nbsp\n",
      "AFTER : café “smart quotes” with nbsp\n"
     ]
    }
   ],
   "source": [
    "def normalize_unicode(text: str) -> str:\n",
    "    # NFKC: canonicalize compatibility characters (common for scraped/converted text)\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    # Convert non-breaking space to normal space\n",
    "    text = text.replace(\"\\u00A0\", \" \")\n",
    "    return text\n",
    "\n",
    "sample = 'café “smart quotes”\\u00A0with\\u00A0nbsp'\n",
    "print(\"BEFORE:\", sample)\n",
    "print(\"AFTER :\", normalize_unicode(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146a7b31",
   "metadata": {},
   "source": [
    "## 7) Step B — Whitespace normalization\n",
    "\n",
    "Why:\n",
    "- Multiple spaces, tab-indents, and too many blank lines reduce embedding quality\n",
    "- Makes chunks more uniform (later)\n",
    "\n",
    "Typical operations:\n",
    "- Convert tabs → single space\n",
    "- Collapse multiple spaces\n",
    "- Collapse multiple blank lines\n",
    "- Strip trailing spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca30e173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE: 'Hello\\t\\tworld!   This   has   spaces.\\n\\n\\nAnd many blanks.   \\n'\n",
      "AFTER : 'Hello world! This has spaces.\\n\\nAnd many blanks.'\n"
     ]
    }
   ],
   "source": [
    "def normalize_whitespace(text: str) -> str:\n",
    "    text = text.replace(\"\\t\", \" \")\n",
    "    # Remove trailing spaces on each line\n",
    "    text = \"\\n\".join(line.rstrip() for line in text.splitlines())\n",
    "    # Collapse multiple spaces\n",
    "    text = re.sub(r\"[ ]{2,}\", \" \", text)\n",
    "    # Collapse 3+ newlines to just 2 (keeps paragraph breaks)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "messy = \"Hello\\t\\tworld!   This   has   spaces.\\n\\n\\nAnd many blanks.   \\n\"\n",
    "print(\"BEFORE:\", repr(messy))\n",
    "print(\"AFTER :\", repr(normalize_whitespace(messy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839b3508",
   "metadata": {},
   "source": [
    "## 8) Step C — Remove known boilerplate patterns\n",
    "\n",
    "In email-like or corporate docs, you’ll see repeated boilerplate:\n",
    "- legal disclaimers\n",
    "- banners like \"INTERNAL USE ONLY\"\n",
    "- signatures\n",
    "\n",
    "Approach:\n",
    "- Keep a list of regex patterns (case-insensitive)\n",
    "- Remove matching lines\n",
    "\n",
    "> Important: Over-aggressive boilerplate removal can delete useful info.\n",
    "So keep rules explicit + versioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d6cb588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL (snippet):\n",
      "Report Title: Quarterly Summary\n",
      "\n",
      "\t\tThis line starts with tabs.\n",
      "\n",
      "\n",
      "\n",
      "Multiple blank lines above.\n",
      "\n",
      "• Bullet 1\n",
      "• Bullet 2\n",
      "\n",
      "Footer: Company Confidential\n",
      "Footer: Company Confidential\n",
      "Footer: Company Confidential\n",
      "\n",
      "\n",
      "CLEANED (snippet):\n",
      "Report Title: Quarterly Summary\n",
      "\n",
      "\t\tThis line starts with tabs.\n",
      "\n",
      "\n",
      "\n",
      "Multiple blank lines above.\n",
      "\n",
      "• Bullet 1\n",
      "• Bullet 2\n",
      "\n",
      "Footer: Company Confidential\n",
      "Footer: Company Confidential\n",
      "Footer: Company Confidential\n"
     ]
    }
   ],
   "source": [
    "BOILERPLATE_PATTERNS = [\n",
    "    r\"^Disclaimer:.*confidential\\.?$\",\n",
    "    r\"^ACME SUPPORT PORTAL — INTERNAL USE ONLY$\",\n",
    "    r\"^-{5,}$\",  # long separator lines\n",
    "]\n",
    "\n",
    "compiled_bp = [re.compile(p, flags=re.IGNORECASE) for p in BOILERPLATE_PATTERNS]\n",
    "\n",
    "def remove_boilerplate_lines(text: str, patterns=compiled_bp) -> str:\n",
    "    kept_lines = []\n",
    "    for line in text.splitlines():\n",
    "        if any(p.match(line.strip()) for p in patterns):\n",
    "            continue\n",
    "        kept_lines.append(line)\n",
    "    return \"\\n\".join(kept_lines)\n",
    "\n",
    "demo = docs[1].page_content\n",
    "print(\"ORIGINAL (snippet):\")\n",
    "print(demo[:220])\n",
    "print(\"\\nCLEANED (snippet):\")\n",
    "print(remove_boilerplate_lines(demo)[:220])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a049c1",
   "metadata": {},
   "source": [
    "## 9) Step D — Remove repeated headers/footers (heuristic)\n",
    "\n",
    "This is super common in PDFs:\n",
    "- Each page repeats the same header & footer\n",
    "- When you load pages, those lines appear again and again\n",
    "\n",
    "Heuristic strategy:\n",
    "1. Split into lines\n",
    "2. Count frequency of each non-trivial line across the document (or across pages)\n",
    "3. Remove lines that appear too often (above a threshold)\n",
    "\n",
    "We’ll demonstrate a simple version that works well for many cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bfc5f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:\n",
      " ACME SUPPORT PORTAL — INTERNAL USE ONLY\n",
      "------------------------------------------\n",
      "\n",
      "FAQ:\n",
      "1) Reset password — go to Settings → Security.\n",
      "2) Contact support at support@example.com\n",
      "\n",
      "Disclaimer: This email and any attachments are confidential.\n",
      "Disclaimer: This email and any attachments are confidential.\n",
      "\n",
      "ACME SUPPORT PORTAL — INTERNAL USE ONLY\n",
      "Page 2\n",
      "\n",
      "\n",
      "AFTER:\n",
      " ------------------------------------------\n",
      "\n",
      "FAQ:\n",
      "1) Reset password — go to Settings → Security.\n",
      "2) Contact support at support@example.com\n",
      "\n",
      "\n",
      "Page 2\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def remove_repeated_lines(text: str, min_line_len: int = 10, freq_threshold: float = 0.30) -> str:\n",
    "    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "    if not lines:\n",
    "        return text\n",
    "\n",
    "    # Count line frequencies\n",
    "    counts = Counter(ln for ln in lines if len(ln) >= min_line_len)\n",
    "    total = len(lines)\n",
    "    # Identify lines repeated too frequently\n",
    "    repeated = {ln for ln, c in counts.items() if c / total >= freq_threshold}\n",
    "\n",
    "    # Filter them out\n",
    "    new_lines = []\n",
    "    for ln in text.splitlines():\n",
    "        s = ln.strip()\n",
    "        if s and s in repeated:\n",
    "            continue\n",
    "        new_lines.append(ln)\n",
    "    return \"\\n\".join(new_lines).strip()\n",
    "\n",
    "text_with_repeats = docs[2].page_content\n",
    "print(\"BEFORE:\\n\", text_with_repeats)\n",
    "print(\"\\nAFTER:\\n\", remove_repeated_lines(text_with_repeats, min_line_len=8, freq_threshold=0.20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfe21a0",
   "metadata": {},
   "source": [
    "## 10) Step E — Deduplicate consecutive duplicate lines\n",
    "\n",
    "Sometimes exports repeat the same footer line multiple times consecutively.\n",
    "\n",
    "We'll remove **consecutive duplicates** while keeping paragraphs intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9b99b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE: Footer: Company Confidential\n",
      "Footer: Company Confidential\n",
      "Footer: Company Confidential\n",
      "\n",
      "Hello\n",
      "Hello\n",
      "World\n",
      "AFTER : Footer: Company Confidential\n",
      "\n",
      "Hello\n",
      "World\n"
     ]
    }
   ],
   "source": [
    "def dedupe_consecutive_lines(text: str) -> str:\n",
    "    out = []\n",
    "    prev = None\n",
    "    for ln in text.splitlines():\n",
    "        if prev is not None and ln.strip() and ln.strip() == prev.strip():\n",
    "            continue\n",
    "        out.append(ln)\n",
    "        prev = ln\n",
    "    return \"\\n\".join(out).strip()\n",
    "\n",
    "x = \"Footer: Company Confidential\\nFooter: Company Confidential\\nFooter: Company Confidential\\n\\nHello\\nHello\\nWorld\"\n",
    "print(\"BEFORE:\", x)\n",
    "print(\"AFTER :\", dedupe_consecutive_lines(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b9cc97",
   "metadata": {},
   "source": [
    "## 11) Build a cleaning pipeline (compose steps)\n",
    "\n",
    "We’ll build:\n",
    "- `clean_text(text)` → returns cleaned text\n",
    "- `clean_documents(docs)` → returns cleaned `Document` objects (same metadata)\n",
    "\n",
    "**Production tip:**\n",
    "Add cleaning metadata like:\n",
    "- `cleaning_version`\n",
    "- `original_char_count`\n",
    "- `clean_char_count`\n",
    "so you can debug later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "754b42b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: demo_docs/doc1.txt\n",
      "\n",
      "--- BEFORE (first 250 chars) ---\n",
      " ACME SUPPORT PORTAL — INTERNAL USE ONLY\n",
      "------------------------------------------\n",
      "\n",
      "Hello   team,\n",
      "\n",
      "This   is    a    test document.  \n",
      "\n",
      "It contains    extra spaces,    odd line breaks,\n",
      "and some unicode like café, naïve, and “smart quotes”.\n",
      "\n",
      "ACME SUPPO\n",
      "\n",
      "--- AFTER  (first 250 chars) ---\n",
      " Hello team,\n",
      "\n",
      "This is a test document.\n",
      "\n",
      "It contains extra spaces, odd line breaks,\n",
      "and some unicode like café, naïve, and “smart quotes”.\n",
      "\n",
      "Page 1\n",
      "\n",
      "METADATA: {'source': 'demo_docs/doc1.txt', 'cleaning_version': 'v1.0', 'original_char_count': 287, 'clean_char_count': 144}\n"
     ]
    }
   ],
   "source": [
    "CLEANING_VERSION = \"v1.0\"\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = normalize_unicode(text)\n",
    "    text = remove_boilerplate_lines(text)\n",
    "    text = dedupe_consecutive_lines(text)\n",
    "    text = normalize_whitespace(text)\n",
    "    # Optional repeated-line removal (tune for your corpus)\n",
    "    text = remove_repeated_lines(text, min_line_len=10, freq_threshold=0.25)\n",
    "    text = normalize_whitespace(text)\n",
    "    return text\n",
    "\n",
    "def clean_documents(documents: List[Document]) -> List[Document]:\n",
    "    cleaned_docs = []\n",
    "    for d in documents:\n",
    "        original = d.page_content\n",
    "        cleaned = clean_text(original)\n",
    "\n",
    "        new_meta = dict(d.metadata)\n",
    "        new_meta.update({\n",
    "            \"cleaning_version\": CLEANING_VERSION,\n",
    "            \"original_char_count\": len(original),\n",
    "            \"clean_char_count\": len(cleaned),\n",
    "        })\n",
    "        cleaned_docs.append(Document(page_content=cleaned, metadata=new_meta))\n",
    "    return cleaned_docs\n",
    "\n",
    "cleaned_docs = clean_documents(docs)\n",
    "\n",
    "# Show diff-like preview for one doc\n",
    "i = 0\n",
    "print(\"SOURCE:\", cleaned_docs[i].metadata.get(\"source\"))\n",
    "print(\"\\n--- BEFORE (first 250 chars) ---\\n\", docs[i].page_content[:250])\n",
    "print(\"\\n--- AFTER  (first 250 chars) ---\\n\", cleaned_docs[i].page_content[:250])\n",
    "print(\"\\nMETADATA:\", cleaned_docs[i].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd28a71d",
   "metadata": {},
   "source": [
    "## 12) Quick quality checks (recommended)\n",
    "\n",
    "Before you proceed to chunking/embeddings (next notebook sections), do checks like:\n",
    "- How many empty docs after cleaning?\n",
    "- Average size reduced?\n",
    "- Any doc became too small?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8385aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW STATS   : {'count': 3, 'empty_count': 0, 'min_chars': 205, 'max_chars': 349, 'avg_chars': 280.3333333333333}\n",
      "CLEAN STATS : {'count': 3, 'empty_count': 0, 'min_chars': 12, 'max_chars': 144, 'avg_chars': 99.66666666666667}\n"
     ]
    }
   ],
   "source": [
    "def stats(documents: List[Document]) -> Dict[str, Any]:\n",
    "    lengths = [len(d.page_content) for d in documents]\n",
    "    return {\n",
    "        \"count\": len(documents),\n",
    "        \"empty_count\": sum(1 for l in lengths if l == 0),\n",
    "        \"min_chars\": min(lengths) if lengths else 0,\n",
    "        \"max_chars\": max(lengths) if lengths else 0,\n",
    "        \"avg_chars\": sum(lengths) / len(lengths) if lengths else 0,\n",
    "    }\n",
    "\n",
    "print(\"RAW STATS   :\", stats(docs))\n",
    "print(\"CLEAN STATS :\", stats(cleaned_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec7d5f4",
   "metadata": {},
   "source": [
    "## 13) What you achieved (and what comes next)\n",
    "\n",
    "✅ You now have:\n",
    "- A folder ingested into LangChain `Document` objects\n",
    "- Metadata preserved for tracing and filtering\n",
    "- A practical cleaning pipeline you can reuse in real projects\n",
    "\n",
    "Next steps (when you’re ready):\n",
    "- Chunking / splitting (token-aware)\n",
    "- Embeddings + Vector DB insertion\n",
    "- Retrieval + rerank"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenticai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
