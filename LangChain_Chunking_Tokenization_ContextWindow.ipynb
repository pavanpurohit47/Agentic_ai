{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e4533fc",
   "metadata": {},
   "source": [
    "# LangChain Notebook (Part 2): Chunking + Tokenization + Context Window (Production mindset)\n",
    "\n",
    "This notebook builds on the previous one:\n",
    "\n",
    "✅ **Part 1 notebook**: *Readers + Document object + Cleaning*  \n",
    "You created `docs` and `cleaned_docs` (`List[Document]`) and preserved metadata.\n",
    "\n",
    "In this notebook, you will learn:\n",
    "1) **Chunking strategies** (why, when, how)  \n",
    "2) **Tokenization** (how to count tokens)  \n",
    "3) **Context window budgeting** (how many chunks can you fit into a prompt)\n",
    "\n",
    "---\n",
    "\n",
    "## How to use this notebook\n",
    "\n",
    "### Option A (recommended)\n",
    "Run Part 1 notebook first, then run this notebook.  \n",
    "You can re-run the “Load + Clean” cell below to regenerate `cleaned_docs` exactly like Part 1.\n",
    "\n",
    "### Option B\n",
    "If you already have `cleaned_docs` in memory (same kernel/session), skip the load/clean cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb1d8ae",
   "metadata": {},
   "source": [
    "## 0) Install & Imports\n",
    "\n",
    "We will use:\n",
    "- `langchain_text_splitters` (splitters)\n",
    "- `tiktoken` (token counting; widely used for OpenAI-style BPE tokenization)\n",
    "\n",
    "If something is missing, run the install cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d479de87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready ✅\n"
     ]
    }
   ],
   "source": [
    "# If needed (run once)\n",
    "# %pip install -U langchain langchain-core langchain-community langchain-text-splitters tiktoken pypdf unstructured python-dotenv\n",
    "\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "\n",
    "# Splitters\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, TokenTextSplitter\n",
    "\n",
    "# Token counter\n",
    "try:\n",
    "    import tiktoken\n",
    "except Exception:\n",
    "    tiktoken = None\n",
    "\n",
    "print(\"Ready ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2a8a8d",
   "metadata": {},
   "source": [
    "## 1) (Reference from Part 1) Re-create `cleaned_docs`\n",
    "\n",
    "This cell reuses the same cleaning approach from Part 1 so this notebook is **standalone**.\n",
    "\n",
    "If you already have `cleaned_docs`, you can skip this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc2cea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- DEMO DATA (same idea as Part 1) ----------\n",
    "DATA_DIR = Path(\"demo_docs\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Create sample docs if folder is empty\n",
    "if not any(DATA_DIR.glob(\"*.txt\")):\n",
    "    (DATA_DIR / \"doc1.txt\").write_text(\n",
    "    \"\"\"ACME SUPPORT PORTAL — INTERNAL USE ONLY\n",
    "------------------------------------------\n",
    "\n",
    "Hello   team,\n",
    "\n",
    "This   is    a    test document.  \n",
    "\n",
    "It contains    extra spaces,    odd line breaks,\n",
    "and some unicode like café, naïve, and “smart quotes”.\n",
    "\n",
    "ACME SUPPORT PORTAL — INTERNAL USE ONLY\n",
    "Page 1\n",
    "\"\"\", encoding=\"utf-8\")\n",
    "\n",
    "    (DATA_DIR / \"doc2.txt\").write_text(\n",
    "    \"\"\"ACME SUPPORT PORTAL — INTERNAL USE ONLY\n",
    "------------------------------------------\n",
    "\n",
    "FAQ:\n",
    "1) Reset password — go to Settings → Security.\n",
    "2) Contact support at support@example.com\n",
    "\n",
    "Disclaimer: This email and any attachments are confidential.\n",
    "Disclaimer: This email and any attachments are confidential.\n",
    "\n",
    "ACME SUPPORT PORTAL — INTERNAL USE ONLY\n",
    "Page 2\n",
    "\"\"\", encoding=\"utf-8\")\n",
    "\n",
    "    (DATA_DIR / \"doc3.txt\").write_text(\n",
    "    \"\"\"Report Title: Quarterly Summary\n",
    "\n",
    "\\t\\tThis line starts with tabs.\n",
    "\\n\\n\\nMultiple blank lines above.\n",
    "\n",
    "• Bullet 1\n",
    "• Bullet 2\n",
    "\n",
    "Footer: Company Confidential\n",
    "Footer: Company Confidential\n",
    "Footer: Company Confidential\n",
    "\"\"\", encoding=\"utf-8\")\n",
    "\n",
    "print(\"Files:\", [p.name for p in DATA_DIR.glob(\"*.txt\")])\n",
    "\n",
    "# ---------- LOAD (Readers) ----------\n",
    "dir_loader = DirectoryLoader(\n",
    "    str(DATA_DIR),\n",
    "    glob=\"**/*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\": \"utf-8\"}\n",
    ")\n",
    "docs: List[Document] = dir_loader.load()\n",
    "print(\"Loaded docs:\", len(docs))\n",
    "\n",
    "# ---------- CLEANING (from Part 1) ----------\n",
    "BOILERPLATE_PATTERNS = [\n",
    "    r\"^Disclaimer:.*confidential\\.?$\",\n",
    "    r\"^ACME SUPPORT PORTAL — INTERNAL USE ONLY$\",\n",
    "    r\"^-{5,}$\",\n",
    "]\n",
    "\n",
    "compiled_bp = [re.compile(p, flags=re.IGNORECASE) for p in BOILERPLATE_PATTERNS]\n",
    "\n",
    "def normalize_unicode(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = text.replace(\"\\u00A0\", \" \")\n",
    "    return text\n",
    "\n",
    "def normalize_whitespace(text: str) -> str:\n",
    "    text = text.replace(\"\\t\", \" \")\n",
    "    text = \"\\n\".join(line.rstrip() for line in text.splitlines())\n",
    "    text = re.sub(r\"[ ]{2,}\", \" \", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def remove_boilerplate_lines(text: str, patterns=compiled_bp) -> str:\n",
    "    kept_lines = []\n",
    "    for line in text.splitlines():\n",
    "        if any(p.match(line.strip()) for p in patterns):\n",
    "            continue\n",
    "        kept_lines.append(line)\n",
    "    return \"\\n\".join(kept_lines)\n",
    "\n",
    "def dedupe_consecutive_lines(text: str) -> str:\n",
    "    out = []\n",
    "    prev = None\n",
    "    for ln in text.splitlines():\n",
    "        if prev is not None and ln.strip() and ln.strip() == prev.strip():\n",
    "            continue\n",
    "        out.append(ln)\n",
    "        prev = ln\n",
    "    return \"\\n\".join(out).strip()\n",
    "\n",
    "from collections import Counter\n",
    "def remove_repeated_lines(text: str, min_line_len: int = 10, freq_threshold: float = 0.25) -> str:\n",
    "    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "    if not lines:\n",
    "        return text\n",
    "    counts = Counter(ln for ln in lines if len(ln) >= min_line_len)\n",
    "    total = len(lines)\n",
    "    repeated = {ln for ln, c in counts.items() if c / total >= freq_threshold}\n",
    "    new_lines = []\n",
    "    for ln in text.splitlines():\n",
    "        s = ln.strip()\n",
    "        if s and s in repeated:\n",
    "            continue\n",
    "        new_lines.append(ln)\n",
    "    return \"\\n\".join(new_lines).strip()\n",
    "\n",
    "CLEANING_VERSION = \"v1.0\"\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = normalize_unicode(text)\n",
    "    text = remove_boilerplate_lines(text)\n",
    "    text = dedupe_consecutive_lines(text)\n",
    "    text = normalize_whitespace(text)\n",
    "    text = remove_repeated_lines(text, min_line_len=10, freq_threshold=0.25)\n",
    "    text = normalize_whitespace(text)\n",
    "    return text\n",
    "\n",
    "def clean_documents(documents: List[Document]) -> List[Document]:\n",
    "    cleaned_docs = []\n",
    "    for d in documents:\n",
    "        original = d.page_content\n",
    "        cleaned = clean_text(original)\n",
    "        new_meta = dict(d.metadata)\n",
    "        new_meta.update({\n",
    "            \"cleaning_version\": CLEANING_VERSION,\n",
    "            \"original_char_count\": len(original),\n",
    "            \"clean_char_count\": len(cleaned),\n",
    "        })\n",
    "        cleaned_docs.append(Document(page_content=cleaned, metadata=new_meta))\n",
    "    return cleaned_docs\n",
    "\n",
    "cleaned_docs: List[Document] = clean_documents(docs)\n",
    "print(\"Cleaned docs:\", len(cleaned_docs))\n",
    "\n",
    "# Preview one\n",
    "print(\"\\nSOURCE:\", cleaned_docs[0].metadata.get(\"source\"))\n",
    "print(cleaned_docs[0].page_content[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aeeed1",
   "metadata": {},
   "source": [
    "# Part A — Chunking (in depth)\n",
    "\n",
    "## Why chunking exists\n",
    "LLMs can’t ingest unlimited text. For RAG, you typically:\n",
    "1) Load → clean → split into chunks\n",
    "2) Embed chunks → store in vector DB\n",
    "3) Retrieve top-k chunks → put in prompt\n",
    "\n",
    "Chunking impacts:\n",
    "- **Recall**: smaller chunks can match more precisely\n",
    "- **Precision**: too small may lose context\n",
    "- **Cost & latency**: bigger chunks increase prompt tokens\n",
    "- **Faithfulness**: better chunking → fewer hallucinations\n",
    "\n",
    "---\n",
    "\n",
    "## What makes a “good chunk”?\n",
    "A good chunk should be:\n",
    "- **Semantically coherent** (not cutting mid-sentence if possible)\n",
    "- **Not too long** (token budget)\n",
    "- **Not too short** (enough context to answer)\n",
    "- Overlap only as needed (avoid excessive redundancy)\n",
    "\n",
    "We’ll explore with `RecursiveCharacterTextSplitter` and token-aware splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9dd30b",
   "metadata": {},
   "source": [
    "## 2) Recursive Character Chunking (most commonly used)\n",
    "\n",
    "This splitter tries multiple separators in order, like:\n",
    "- `\\n\\n` (paragraph)\n",
    "- `\\n` (line)\n",
    "- `\" \"` (space)\n",
    "- fallback to hard cuts\n",
    "\n",
    "Good baseline for mixed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727491b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_with_recursive(\n",
    "    documents: List[Document],\n",
    "    chunk_size: int = 500,\n",
    "    chunk_overlap: int = 80,\n",
    ") -> List[Document]:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = splitter.split_documents(documents)\n",
    "    return chunks\n",
    "\n",
    "chunks_500 = chunk_with_recursive(cleaned_docs, chunk_size=500, chunk_overlap=80)\n",
    "print(\"Chunks created:\", len(chunks_500))\n",
    "print(\"Example chunk metadata:\", chunks_500[0].metadata)\n",
    "print(\"Example chunk text:\\n\", chunks_500[0].page_content[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8e3fc7",
   "metadata": {},
   "source": [
    "## 3) Compare chunk sizes and overlap (experiment)\n",
    "\n",
    "You’ll see how:\n",
    "- chunk size affects number of chunks\n",
    "- overlap increases total text duplication (cost)\n",
    "\n",
    "Rule of thumb starting points:\n",
    "- 300–800 characters (or ~150–400 tokens) for QA docs\n",
    "- Overlap 10–20% of chunk size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40730d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_stats(chunks: List[Document]) -> Dict[str, Any]:\n",
    "    lens = [len(c.page_content) for c in chunks]\n",
    "    return {\n",
    "        \"chunks\": len(chunks),\n",
    "        \"min_chars\": min(lens) if lens else 0,\n",
    "        \"max_chars\": max(lens) if lens else 0,\n",
    "        \"avg_chars\": sum(lens)/len(lens) if lens else 0,\n",
    "    }\n",
    "\n",
    "for size, overlap in [(200, 40), (500, 80), (1000, 120)]:\n",
    "    ch = chunk_with_recursive(cleaned_docs, chunk_size=size, chunk_overlap=overlap)\n",
    "    print(f\"chunk_size={size}, overlap={overlap} ->\", chunk_stats(ch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb445211",
   "metadata": {},
   "source": [
    "## 4) Metadata hygiene for chunks\n",
    "\n",
    "When chunking, you want chunk-level metadata like:\n",
    "- `source` (file)\n",
    "- `start_index` (where chunk begins in original text)\n",
    "- `chunk_id` (unique id)\n",
    "- optional: `doc_id`, `page` for PDFs\n",
    "\n",
    "We'll add `chunk_id` in a deterministic way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f547d322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def attach_chunk_ids(chunks: List[Document]) -> List[Document]:\n",
    "    out = []\n",
    "    for idx, c in enumerate(chunks):\n",
    "        # Deterministic ID using source + start_index + hash(text)\n",
    "        source = str(c.metadata.get(\"source\", \"unknown\"))\n",
    "        start = str(c.metadata.get(\"start_index\", 0))\n",
    "        h = hashlib.sha1(c.page_content.encode(\"utf-8\")).hexdigest()[:12]\n",
    "        chunk_id = f\"{Path(source).name}:{start}:{h}:{idx}\"\n",
    "        meta = dict(c.metadata)\n",
    "        meta[\"chunk_id\"] = chunk_id\n",
    "        out.append(Document(page_content=c.page_content, metadata=meta))\n",
    "    return out\n",
    "\n",
    "chunks_500 = attach_chunk_ids(chunks_500)\n",
    "print(\"Chunk id example:\", chunks_500[0].metadata[\"chunk_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34995610",
   "metadata": {},
   "source": [
    "# Part B — Tokenization & Context Window (in depth)\n",
    "\n",
    "## Why tokenization matters\n",
    "LLMs operate in **tokens**, not characters.\n",
    "\n",
    "Your total token budget includes:\n",
    "- System + user prompt tokens\n",
    "- Retrieved context tokens (all chunks you stuff into prompt)\n",
    "- Tool/function calling overhead (if any)\n",
    "- Output tokens (`max_tokens` / `max_new_tokens`)\n",
    "\n",
    "If you exceed the context window, the model will:\n",
    "- truncate inputs (losing context)\n",
    "- error (some APIs)\n",
    "- degrade quality\n",
    "\n",
    "---\n",
    "\n",
    "We’ll implement:\n",
    "- token counting for strings\n",
    "- estimating chunk token sizes\n",
    "- computing “how many chunks can fit” for a given context window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ac5ace",
   "metadata": {},
   "source": [
    "## 5) Token counting with `tiktoken`\n",
    "\n",
    "`tiktoken` is commonly used for GPT-style tokenization.\n",
    "If you’re using other models (Llama/Mistral), token counts differ, but this is still a very useful approximation for budgeting.\n",
    "\n",
    "If `tiktoken` is not installed, install it:\n",
    "```python\n",
    "%pip install -U tiktoken\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2d6a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_encoder(model_hint: str = \"gpt-4o-mini\"):\n",
    "    if tiktoken is None:\n",
    "        raise ImportError(\"tiktoken is not installed. Run: %pip install -U tiktoken\")\n",
    "    try:\n",
    "        return tiktoken.encoding_for_model(model_hint)\n",
    "    except Exception:\n",
    "        return tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def count_tokens(text: str, model_hint: str = \"gpt-4o-mini\") -> int:\n",
    "    enc = get_token_encoder(model_hint)\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "sample_text = cleaned_docs[0].page_content\n",
    "print(\"Sample chars:\", len(sample_text))\n",
    "print(\"Approx tokens:\", count_tokens(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c27fe4",
   "metadata": {},
   "source": [
    "## 6) Token-aware chunking (`TokenTextSplitter`)\n",
    "\n",
    "Character chunking ≠ token chunking.\n",
    "\n",
    "`TokenTextSplitter` ensures each chunk is around **N tokens**, which is safer for context windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2f46c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_with_tokens(\n",
    "    documents: List[Document],\n",
    "    chunk_size_tokens: int = 200,\n",
    "    chunk_overlap_tokens: int = 40,\n",
    "    model_hint: str = \"gpt-4o-mini\"\n",
    ") -> List[Document]:\n",
    "    # TokenTextSplitter uses a tokenizer internally\n",
    "    splitter = TokenTextSplitter(\n",
    "        chunk_size=chunk_size_tokens,\n",
    "        chunk_overlap=chunk_overlap_tokens,\n",
    "        encoding_name=\"cl100k_base\",  # stable default; ok even if model differs\n",
    "    )\n",
    "    return splitter.split_documents(documents)\n",
    "\n",
    "token_chunks = chunk_with_tokens(cleaned_docs, chunk_size_tokens=120, chunk_overlap_tokens=20)\n",
    "print(\"Token-chunks:\", len(token_chunks))\n",
    "\n",
    "# Show actual token sizes\n",
    "sizes = [count_tokens(c.page_content) for c in token_chunks[:10]]\n",
    "sizes, {\"min\": min(sizes), \"max\": max(sizes), \"avg\": sum(sizes)/len(sizes)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb0e0ec",
   "metadata": {},
   "source": [
    "## 7) Context window budgeting\n",
    "\n",
    "You usually allocate budget like this:\n",
    "\n",
    "- **Model context window**: e.g., 8k / 16k / 32k / 128k tokens\n",
    "- Reserve tokens for:\n",
    "  - Instructions + question (e.g., 300–800 tokens)\n",
    "  - The answer (e.g., 400–1200 tokens)\n",
    "- Remaining is retrieval context budget\n",
    "\n",
    "We’ll write a helper:\n",
    "- estimate how many chunks (top-k) can fit in remaining budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a5abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_k_fit(\n",
    "    chunks: List[Document],\n",
    "    question: str,\n",
    "    system_prompt: str,\n",
    "    context_window: int = 8192,\n",
    "    reserved_for_answer: int = 800,\n",
    "    model_hint: str = \"gpt-4o-mini\",\n",
    ") -> Dict[str, Any]:\n",
    "    prompt_tokens = count_tokens(system_prompt + \"\\n\" + question, model_hint=model_hint)\n",
    "    remaining = context_window - reserved_for_answer - prompt_tokens\n",
    "    if remaining < 0:\n",
    "        return {\n",
    "            \"prompt_tokens\": prompt_tokens,\n",
    "            \"remaining_for_context\": remaining,\n",
    "            \"k_fit\": 0,\n",
    "            \"note\": \"Prompt + reserved answer already exceed context window.\"\n",
    "        }\n",
    "\n",
    "    token_sizes = [count_tokens(c.page_content, model_hint=model_hint) for c in chunks]\n",
    "    total = 0\n",
    "    k = 0\n",
    "    for sz in token_sizes:\n",
    "        if total + sz > remaining:\n",
    "            break\n",
    "        total += sz\n",
    "        k += 1\n",
    "\n",
    "    return {\n",
    "        \"context_window\": context_window,\n",
    "        \"prompt_tokens\": prompt_tokens,\n",
    "        \"reserved_for_answer\": reserved_for_answer,\n",
    "        \"remaining_for_context\": remaining,\n",
    "        \"k_fit\": k,\n",
    "        \"context_tokens_used\": total,\n",
    "    }\n",
    "\n",
    "system_prompt = \"You are a helpful assistant. Answer only from the provided context.\"\n",
    "question = \"How do I reset my password and contact support?\"\n",
    "\n",
    "# Use token-based chunks for realistic budgeting\n",
    "token_chunks = chunk_with_tokens(cleaned_docs, chunk_size_tokens=180, chunk_overlap_tokens=30)\n",
    "\n",
    "estimate_k_fit(\n",
    "    chunks=token_chunks,\n",
    "    question=question,\n",
    "    system_prompt=system_prompt,\n",
    "    context_window=8192,\n",
    "    reserved_for_answer=800\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a237c31d",
   "metadata": {},
   "source": [
    "## 8) Practical recipe (recommended defaults)\n",
    "\n",
    "For many enterprise RAG Q&A cases:\n",
    "- **Token chunk size**: 200–400 tokens\n",
    "- **Overlap**: 10–20% (20–80 tokens)\n",
    "- Retrieve: `top_k = 10–30` (recall)\n",
    "- Rerank: keep `top_k = 3–8` (precision)\n",
    "- Prompt stuffing: fit `3–8` chunks depending on context window\n",
    "\n",
    "Next notebook (if you want):\n",
    "✅ Retrieval + MMR + Rerank + Prompt packing + LLM call"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenticai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
